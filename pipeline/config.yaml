
# config parameters for MetaDIVE run X.

# These are read by snakemake as a list 
samples:
  Sample0:
    - /filepath/to/raw/reads/MetaDIVE/pipeline/raws/Sample0_R1.fastq.gz
    - /filepath/to/raw/reads/MetaDIVE/pipeline/raws/Sample0_R2.fastq.gz
  Sample1:
    - /filepath/to/raw/reads/MetaDIVE/pipeline/raws/Sample1_R1.fastq.gz
    - /filepath/to/raw/reads/MetaDIVE/pipeline/raws/Sample1_R2.fastq.gz
  Sample2:
    - /filepath/to/raw/reads/MetaDIVE/pipeline/raws/Sample2_R1.fastq.gz
    - /filepath/to/raw/reads/MetaDIVE/pipeline/raws/Sample2_R2.fastq.gz
  Sample3:
    - /filepath/to/raw/reads/MetaDIVE/pipeline/raws/Sample3_R1.fastq.gz
    - /filepath/to/raw/reads/MetaDIVE/pipeline/raws/Sample3_R2.fastq.gz
  Sample4:
    - /filepath/to/raw/reads/MetaDIVE/pipeline/raws/Sample4_R1.fastq.gz
    - /filepath/to/raw/reads/MetaDIVE/pipeline/raws/Sample4_R2.fastq.gz
  Sample5:
    - /filepath/to/raw/reads/MetaDIVE/pipeline/raws/Sample5_R1.fastq.gz
    - /filepath/to/raw/reads/MetaDIVE/pipeline/raws/Sample5_R2.fastq.gz
  Sample6:
    - /filepath/to/raw/reads/MetaDIVE/pipeline/raws/Sample6_R1.fastq.gz
    - /filepath/to/raw/reads/MetaDIVE/pipeline/raws/Sample6_R2.fastq.gz
  Sample7:
    - /filepath/to/raw/reads/MetaDIVE/pipeline/raws/Sample7_R1.fastq.gz
    - /filepath/to/raw/reads/MetaDIVE/pipeline/raws/Sample7_R2.fastq.gz
  Sample8:
    - /filepath/to/raw/reads/MetaDIVE/pipeline/raws/Sample8_R1.fastq.gz
    - /filepath/to/raw/reads/MetaDIVE/pipeline/raws/Sample8_R2.fastq.gz
  Sample9:
    - /filepath/to/raw/reads/MetaDIVE/pipeline/raws/Sample9_R1.fastq.gz
    - /filepath/to/raw/reads/MetaDIVE/pipeline/raws/Sample9_R2.fastq.gz
  Sample10:
    - /filepath/to/raw/reads/MetaDIVE/pipeline/raws/Sample10_R1.fastq.gz
    - /filepath/to/raw/reads/MetaDIVE/pipeline/raws/Sample10_R2.fastq.gz


# Program directory (working directory to output results and where the rules/scripts folders are along with the snakefiles and config files)
program_dir: "/filepath/to/raw/reads/MetaDIVE/pipeline/"


# This section onwards will define key variables that influence the analysis pipeline directions and rules called

# Sensitivity setting. Acts as a proxy for how much memory avaliable for the analysis. This is the amount of memory passed to snakemake in the run_snakemake script
# Recommended memory ranges per settings are as described as below:
# VHigh = 180GB or more 
# High = 130-180GB  
# Medium = 90-130GB 
# Medium-Low = 60-90GB
# Low = 40-60 GB # Note this pipeline currently has had limited testing at such low memory, large read datasets e.g., Nextseq/novaseq require far more memory than this to analyse. Many programs used in metagenomics require more than 40GB to run for just one sample. Running with so little may cause progams within the pipeline to crash
# Ultra_low = <40GB   # Note this pipeline currently isn't tested at such low memory. Many individual programs used in metagenomics require more than 40GB to run for just one sample. Running with so little memory may cause programs within the pipeline to crash

Sensitivity: 'Medium'

# Fastp filtering and trimming

minimum_length_filter_fastp: 85
complexity_threshold: 10
front_window_cutsize: 4
# base quality score min (phred score)
min_qual_filter: 16
# min average quality read filter.
min_qual_filter_avg_read: 19
# required minimum running average quality score for the 4 base window
min_qual_window: 20





# Assembly choices
# options at the moment. Trinity, or Megahit
# If running Low/medium low sensitivity. It is recommended to run Megahit only.

Assembly_choice: 'Megahit'


# These below two settings are currently only implemented for Trinity runs. As trinity can create up to a million intermediate files of near zero size
# this can crash a computer/compute cluster by breaking its read/write capacity/rate. One solution to this is to get trinity to 
# write to store these files in memory instead of physically writing them. As they are so small typically all files can be stored on
# only ~12GB of memory and can create significant speed ups to a trinity run.
# This requires this to be possible on your HPC system if you use it. It is currently hard coded to sym link (ln -s ) to the memory 
# directory in the snakemake rule.
# consider whether you have a global variable for the memory directory  e.g., $MEMDIR that you can put in to the file directly, or
# if there is a hard code path for where the memdir is if there is one. if there is neither of these, utilising  the memdir will not 
# be feasible for you with the current shell scripts used. If you have any ideas on how to define the $MEMDIR in specific/general use
# cases in shell script that can be implemented in later iterations

Use_memory_as_storage: 'yes'
Memory_directory_location: $MEMDIR




# Assembly contig minimum size: This is the minimum size in base pairs for assembled genome fragments created from the paired end reads (smaller size allows for more reads to be assembled
# but smaller size requires significantly more memory and extends analysis time. Lastly very small sized contigs can lead to increased false positives).
Assembly_size: '301'


# Diamond sensitivity settings. This setting strongly influences how long the entire pipeline takes as the highest sensitivity levels can result in the single Diamond contig blastx 
# taking the same length as all other steps in the pipeline combined. 
# Based on the approximations from the Diamond manual page. 
# fast= >90% amino acid identity hits
# mid-sensitive is partway between fast and sensitive e.g., ~60%.
# sensitive is for >40% amino acid sensitivity
# more-sensitive is the same as sensitive but with no masking so repeat sequences are better captured 
# very-sensitive is for sequences <40% amino acid sensitivity
# ultra-sensitive is more sensitive than very-sensitive. 
# If you are in a rush and after only known, pathogenic viruses, fast is sufficient. 
# If you are working with species where fewer viruses are classified but it is likely you are looking at viruses from a genus that is known I would recommend mid-sensitive 
# If you are looking to detect very divergent viruses from either poorly classified genera/families, or unclassified viruses I would recommend sensitive. 
# For anything more diverged, I would recommend running geNomad instead as this has a lower false positive chance and is more efficient/faster than very-sensitive Diamond blastx 
# while having the advantage of picking up far more novel viruses. 
# options are 
# 'fast' 'mid-sensitive', 'sensitive', 'more-sensitive', 'very-sensitive', 'ultra-sensitive'

Diamond_sensitivity_setting: 'mid-sensitive'



# Whether or not to run a false positive check on the contigs assigned to viruses using BLASTn 
Blastn_viral_contig_false_positive_check: 'yes'


# Whether to return all contigs generated which Diamond blast matched to a virus or only those which passed the false positive check for final reads 
# and contigs files generated. This will also change the names of viral assignments to match blastn results preferrentially over blastx results. 
# 'all'/'confirmed'
Final_contigs_returned: 'confirmed'


# Not currently implemented in scripts ## TODO: Add in parameter for NCBI API key to downloadgenome.sh and download_variety_genomes_high_completion_viruses.sh
NCBI_API_KEY: 'none'


# This setting will delete all intermediary files except for the final summaries, reports, returned reads, and quality score reports once the pipeline has finished.  
# To keep all outputs write "no" To delete everything write "yes"
Delete_inter_files: "no"

# Databases and other filepaths (once updated during the first time, these will remain static). All have been tarred and compressed on github or will be built on the spot except the Diamond and blastn databases.
# Diamond and blastn databases were not included as these are very large databases (>100GB each usually) and it is recommended these are installed on an HPC (high performance computer cluster) in a location everyone can access. They are also very common so will likely be present somewhere if routine bioinformatics work
# has been done on your HPC before. 
# Have a look around the HPC and potentially contact your HPC admin to see where they are before downloading them yourself. (note this last part to download the diamond and blastn databases isn't made yet and I may not at all) Otherwise,run Diamond_download.sh, and Diamond_download.sh








#'yes'/'no'
inspecthost: 'yes'


# Whether to filter out host reads. 
#'yes'/'no'
Host_filter: 'yes'

# testing advanced microbiome analysis for rRNA. 
#'yes/no'.
Microbiome_classification: 'yes'

# Weights given to host assignments. This allows for scaling of the results for host identification taking into account a specific marker may not be as resolving for
# one species as it is for another (requires a general understanding of the expected species/genus/family that the host might be to use most effectively). I have 
# set to 3 for CO1 and 1 for LSU and SSU here so CO1 is weighted twice as much as it is a very resolving marker for Eukaryotes (better than 16s, usually, and most commonly for mammals).
# The weighting is relative and so the ratio of the numbers between the markers is how the markers are scaled.
CO1weight: 3
LSUweight: 1
SSUweight: 2

# Whether or not to try and analyse non coding related DNA. yes/no
# This adds a very large time increase into the pipeline and adds a comparatively small benefit. at 36 cores, decently sequenced samples on a p2 nextseq cartridge took
# an additional 1-24 hours to run through (depending on cpu cores) (A goal of a sample every 6-12 hours start to complete is unlikely with this setting on).
# Worst case scenario, a beta tester with a very large number of unassigned contigs turned this on and each sample took 2.5 days to complete this step. Strongly recommend it is only used 
# when working with smaller datasets or when >95% of reads are expected to be depleted due to host.
#'yes'/'no'

DNA_assign_blastn: 'no'







# Whether or not to build viral genomes (consensus with reference) from returned assigned viral genes
# Need to have genome build on for tree building to work
# 'yes'/'no'
Viral_genome_build: 'no'
Viral_genome_tree_building: 'no'

# Whether or not to run genomad viral detection pathways 
# 'yes/no'
Genomad_detect: 'no'


# Whether to run analyses on the raws reads that were not aligned to a contig (requires a long time but can be significantly reduced if Diamond raw viral filt only is set to yes)
# Turn both this and diamond blast to no or to yes together (legacy setting from when multiple raws analysis methods were used, will remove over time)
# 'yes/no'
run_raws: 'yes'

# Whether to do diamond blast on the raws (very long, probably around 15-20% of the whole pipeline, but setting Diamond raw viral filt only significantly reduces this time down to 5-10% of whole pipeline)) Set to yes with the run_raws or set both to no.
#'yes/no'
dodiamond_blast_raws: 'yes'

# Whether to subset the blastn raws to save time (strongly recommended if you have a big dataset of fragmented genomes typical of complex environmental samples. 
# Set to 50000 so a maximum of 50000 raw reads after they have been compressed in cdhit will have blastn run on them. I would recommend not putting this above 100000
# This setting does not mean only 50000 raw reads will be analysed. All raw reads will be analysed with BLASTx (Diamond blastx) if run_raws is set
# The reads_max_filter is applied to the reads detected as viruses either through BLASTx or Kraken2 and after CD-hit has compressed the viral reads to remove duplicates. This means in samples with 
# an expected low-medium viral count, 5000-50000 reads will likely be all potential viral reads and the samples which contain more, reads are first prioritised by how likely
# the read is to be viral (based on Kraken assignment scores)
Raw_reads_max_kraken: 10000
Raw_reads_max: 20000



# Whether to run Diamond blast for raw reads using only a viral reference database within Diamond. Will greatly speed up the analysis of single reads. May lead 
# to an increased rate of false positives but all new false positives are likely to be filtered out at the blastn stage afterward leading to little overall impact
# NOTE: This requires you to have a Diamond database which was built with TaxID information present. 
# Values are yes/no
Diamondrawviralfiltonly: 'yes'

# Whether to attempt to run a much more sensitive Diamond blastx (~40%+ similarity) search on the unassigned reads and contigs targeting previously identified viral species. 
# Currently, contigs are detected at ~50-80% amino acid similarity to a reference depending on diamond settings and reads are detected at ~90% amino acid similarity to reference
# This step will run at sensitive settings using a taxid subset of the blast database which only contains the previously matched against viral species. This approach will cut down
# analysis time for this step by >99.99% but can introduce false positives. This will allow for all single reads to potentially be detected in diverged viruses where previously nothing 
# outside of contigs would normally be found. It also allows for slightly more diverged contigs to be detected e.g., if only an RdRPat 55% similarity was found for a viral species 
# in the first Diamond blast but no other genes were this will allow for potentially less conserved genes from that virus to be detected alongside the RdRP. 
# values are 'yes'/'no'
Divergent_reads_and_contigs_search: 'yes'


# sensitivity settings for the viral species specific diamond blastx. If you want to potentially return more contigs set to more sensitive than the earlier diamond sensitivity
# Recommend you use anything as sensitive or more sensitive than the option 'sensitive' e.g., 'sensitive', 'more-sensitive', 'very-sensitive', 'ultra-sensitive'
# values are  'fast' 'mid-sensitive', 'sensitive', 'more-sensitive', 'very-sensitive', 'ultra-sensitive'
Divergent_reads_and_contigs_sensitivity: 'more-sensitive'



# Final read count threshold. The final combined raws and contigs reads table will be filtered to this minimum read count
# Higher values will result in more likely viruses to be detected but may miss more low frequency viruses
# recommended between 3 and 20 depending on the purpose of the analysis. 
readcountthresh: 3




# Trinity temp path.
# This is incredibly important because even a single Trinity run can hit a disk file number quota maximum. Trinity produces large numbers of near empty files during analysis (from 10k-500k per sample depending on reads analysed)
# Choose a spot which allows for lots of files to be written (you can usually see how much space you have on a shared HPC system with the command `quota -s` ) 
Trinitytemppath: "/filepath/to/raw/reads/MetaDIVE/Trinity_temp/"

# Illumina generic adapters for adapter removal.

Illumina_adapters: "/filepath/to/adapterfile/MetaDIVE/databases/adapters/trueseq_PE_adaptors_shortlist.fa"

#PhiX filter. Saved into databases tar. 
PhiX_genome_index: "/filepath/to/phix/MetaDIVE/databases/Phix_reference/Phix_reference_idx"

# CO1 genomes Bowtie indexed
CO1_genome_index: "/filepath/to/Co1index/MetaDIVE/databases/bowtie2/CO1/CO1_reference_idx"

# CO1 genomes mmseqs2 indexed
CO1_genome_index_mmseq: "/filepath/to/Co1index2/MetaDIVE/databases/CO1/CO1_DB"

#LSU genomes Bowtie indexed.
LSU_genome_index: "/filepath/to/LSUindex/MetaDIVE/databases/bowtie2/LSU/LSU_reference_idx"

# LSU taxonomy map mmseq2 indexed
LSU_genome_index_mmseq: "/filepath/to/LSUindex2/MetaDIVE/databases/LSU/LSU_DB"

# Silva LSU taxonomy map
SSU_genome_index: "/filepath/to/SSUindex/MetaDIVE/databases/bowtie2/SSU/SSU_reference_idx"

# SSU taxonomy map mmseq2
SSU_genome_index_mmseq: "/filepath/to/SSUindex2/MetaDIVE/databases/SSU/SSU_DB"

# Diamond nr database Currently set to general DB on Petrichor HPC CSIRO
diamond_database: "/data/bioref/diamond_db/nr.dmnd"

# Blast nucleotide base. Currently set to general DB on Petrichor HPC CSIRO
blast_nucleotide_database: "/data/bioref/blast/ncbi/nt"

# Kraken database for kraken viral assignment
Kraken_database: "/filepath/to/krakendb/MetaDIVE/databases/krakendb/"

#MMseq database Currently set to general DB on Petrichor HPC CSIRO (Also I think this is redundant now, double check if this is needed after mmseqs2 was removed from nucleotide blast step. 
MMSeq_nucleotide_database: "/path/Irrelevant"

# Accessiontaxa file
Accession_allnamenode: "/filepath/to/Accessions/MetaDIVE/databases/taxonomy/nameNode.sqlite"

# Points to the names and nodes dmp files for use in taxonomizr to insert missing taxids in BLASTs before importing into R
NCBI_reference_dir: "/filepath/to/taxonomy/MetaDIVE/databases/taxonomy"

Genomaddb: "/filepath/to/Genomad/MetaDIVE/databases/genomad/genomad_db"

############################################################################################
############################################################################################
############################################################################################



# File paths. Do not need to be changed. If you want, you can change the right side names in talking marks without crashing the pipeline e.g., "01_FASTP". If you do this and it creates an error
# let me know which you changed and I will fix in the next iteration of the pipeline.

sub_dirs:
  trim_dir: "01_FASTP"
  failed_trim_dir: "01_1_FASTP_FAILED"
  trim_report_dir: "01_2_FASTP_REPORTS"
  PhiX_dir: "02_PHIX_FILTER"
  CO1_dir: "03_CO1_FILTER"
  LSU_dir: "04_LSU_FILTER"
  SSU_dir: "05_SSU_FILTER"
  host_remove_dir: "05B_HOSTREMOVAL_FILTER"
  contig_dir_megahit: "06_CONTIG_ASSEMBLY_MEGAHIT"
  contig_dir_either: "06_CONTIG_ASSEMBLY"
  contig_dir_host_rem: "06B_CONTIG_ASSEMBLY_HOST_REMOVED"
  contig_dir_trinity: "06_CONTIG_ASSEMBLY_TRINITY"
  diamond_dir: "07_DIAMOND_BLAST"
  Kraken_viral_ids_contigs: "07B_KRAKEN_DETECT_VIRAL_CONTIGS"
  contigs_assigned: "08_DIAMOND_BLAST_SUMMARY"
  contigs_assigned_nucl: "09_BLASTN_SUMMARY"
  contigs_assigned_nucl_abundances: "09_BLASTN_SUMMARY/abundances"
  contigs_assigned: "08_CONTIGS_ASSIGNED"
  contig_compiled_assignments: "08_CONTIGS_ASSIGNED/DNA_RNA_combined_assignments" # Potential remove
  raws_to_contigs: "10_RAW_ALIGNMENTS_CONTIGS"
  raws_to_host_contigs: "10B_RAW_ALIGNMENTS_HOST_CONTIGS"
  temp_genome_path: "00_TEMP_GENOME_INDEXES"
  diamond_raws: "11_DIAMOND_BLAST_RAWS"
  contigs_assigned_raw: "11B_DIAMOND_ASSIGNED_RAWS"
  Kraken_viral_ids: "11C_KRAKEN_RAWS_IDENTIFIED"
  raws_blastn_check: "12_BLASTN_RAW_READS_CHECK"
  raws_blastn_r: "12B_BLASTN_RAW_READS_STATISTICS"
  raws_results: "13_RAW_READS_SUMMARY"
  contigs_nucl_false_pos_check: "14_BLASTN_FALSE_POSITIVE_CHECK"
  contigs_nucl_false_pos_check_counts: "15_BLASTN_FALSE_POSITIVE_CHECK_SUMMARY"
  Viral_genomes_present: "16_REFERENCE_VIRAL_GENOMES"
  Viral_genomes_present_summary: "17_REFERENCE_VIRAL_GENOMES_SUMMARY"
  Viral_genomes_present_significant: "18_REFERENCE_VIRAL_GENOMES_SIGNIFICANT_ASSEMBLY"
  Viral_genomes_present_refined_genomes: "18B_COMPLETE_VIRAL_GENOMES_GUIDED_ASSEMBLY"
  Viral_genomes_present_mafft: "18C_MAFFT_ALIGNMENTS_GENERATED_VIRAL_GENOMES"
  Viral_genomes_present_trimal: "18D_TRIMAL_ALIGNMENTS_GENERATED_VIRAL_GENOMES"
  Viral_genomes_present_iqtree: "18E_IQTREE_GENERATED_VIRAL_GENOMES"
  Genomad_viral_ids: "19_GENOMAD_VIRAL_LIKE_CONTIGS"
  Genomad_viral_ids_prep: "19B_SUBSET_CONTIGS_FILES"
  Genomad_viral_diamond: "20_GENOMAD_VIRAL_DIAMOND_HITS"
  Genomad_viral_blastn: "20B_GENOMAD_VIRAL_BLASTN_HITS"
  Genomad_viral_host_rem: "21_GENOMAD_VIRAL_NO_HOST"
  Genomad_viral_blastp: "22_GENOMAD_VIRAL_NO_HOST_BLASTP"
  Genomad_viral_categorise_species: "23_GENOMAD_VIRAL_GENE_CLUSTERS"
  Genomad_viral_binning_prep: "24_METABAT_BINNING_PREP"
  Genomad_viral_binning_metabat: "25_METABAT_BINNING_RESULTS"
  Genomad_metabat_sorted: "26_METABAT_BINS_SORTED"
  Viral_diamond_sensitive_check: "27_DIAMOND_SEARCH_TARGET_VIRUSES"
  Viral_diamond_sensitive_summary: "28_DIAMOND_SUMMARY_TARGET_VIRUSES"
  host_detect_CO1: "03_CO1_FILTER/HOSTREMOVAL/01_STRONG_ALIGN"
  host_detect_LSU: "04_LSU_FILTER/HOSTREMOVAL/01_STRONG_ALIGN"
  host_detect_SSU: "05_SSU_FILTER/HOSTREMOVAL/01_STRONG_ALIGN"
  host_contigs_CO1: "03_CO1_FILTER/HOSTREMOVAL/01a_STRONG_ALIGN_CONTIGS"
  host_contigs_LSU: "04_LSU_FILTER/HOSTREMOVAL/01a_STRONG_ALIGN_CONTIGS"
  host_contigs_SSU: "05_SSU_FILTER/HOSTREMOVAL/01a_STRONG_ALIGN_CONTIGS"
  host_assignLCA_CO1_contigs: "03_CO1_FILTER/HOSTREMOVAL/02A_TAX_ASSIGN_LCA_CONTIGS"
  host_assignLCA_LSU_contigs: "04_LSU_FILTER/HOSTREMOVAL/02A_TAX_ASSIGN_LCA_CONTIGS"
  host_assignLCA_SSU_contigs: "05_SSU_FILTER/HOSTREMOVAL/02A_TAX_ASSIGN_LCA_CONTIGS"
  host_assignLCA_CO1: "03_CO1_FILTER/HOSTREMOVAL/02_TAX_ASSIGN_LCA"
  host_assignLCA_LSU: "04_LSU_FILTER/HOSTREMOVAL/02_TAX_ASSIGN_LCA"
  host_assignLCA_SSU: "05_SSU_FILTER/HOSTREMOVAL/02_TAX_ASSIGN_LCA"
  host_LCA_plots_CO1_contigs: "03_CO1_FILTER/HOSTREMOVAL/03A_LCA_RESULTS_CONTIGS"
  host_LCA_plots_LSU_contigs: "04_LSU_FILTER/HOSTREMOVAL/03A_LCA_RESULTS_CONTIGS"
  host_LCA_plots_SSU_contigs: "05_SSU_FILTER/HOSTREMOVAL/03A_LCA_RESULTS_CONTIGS"
  host_LCA_plots_CO1: "03_CO1_FILTER/HOSTREMOVAL/03_LCA_RESULTS"
  host_LCA_plots_LSU: "04_LSU_FILTER/HOSTREMOVAL/03_LCA_RESULTS"
  host_LCA_plots_SSU: "05_SSU_FILTER/HOSTREMOVAL/03_LCA_RESULTS"
  CO1_contig_blast_dir: "03_CO1_FILTER/HOSTREMOVAL/04_CONTIG_BLASTN_RESULTS"
  LSU_contig_blast_dir: "04_LSU_FILTER/HOSTREMOVAL/04_CONTIG_BLASTN_RESULTS"
  SSU_contig_blast_dir: "05_SSU_FILTER/HOSTREMOVAL/04_CONTIG_BLASTN_RESULTS"
  host_detect_LSU_rapid: "04_LSU_FILTER/HOSTREMOVAL/01_STRONG_ALIGN_RAPID"
  host_detect_SSU_rapid: "05_SSU_FILTER/HOSTREMOVAL/01_STRONG_ALIGN_RAPID"
  host_contigs_LSU_rapid: "04_LSU_FILTER/HOSTREMOVAL/01a_STRONG_ALIGN_CONTIGS_RAPID"
  host_contigs_SSU_rapid: "05_SSU_FILTER/HOSTREMOVAL/01a_STRONG_ALIGN_CONTIGS_RAPID"
  host_assignLCA_rapid_LSU: "04_LSU_FILTER/HOSTREMOVAL/02_TAX_ASSIGN_LCA_RAPID"
  host_assignLCA_rapid_SSU: "05_SSU_FILTER/HOSTREMOVAL/02_TAX_ASSIGN_LCA_RAPID"
  host_LCA_rapid_plots_LSU: "04_LSU_FILTER/HOSTREMOVAL/03_LCA_RESULTS_RAPID"
  host_LCA_rapid_plots_SSU: "05_SSU_FILTER/HOSTREMOVAL/03_LCA_RESULTS_RAPID"
  Summary_results: "99_SUMMARY_RESULTS_INDIVIDUAL_SAMPLES"
  Summary_results2: "99_SUMMARY_RESULTS_COMBINED_SAMPLES_GRAPHS"
  compiled_summary: "99_SUMMARY_RESULTS_INDIVIDUAL_SAMPLES_RETURNED_INDIVIDUAL_READS"
  host_species_genomes: "00A_TOP_HOST_SPECIES_PER_SAMPLE"
  meal_species_id: "00B_TOP_MEAL_SPECIES_PER_SAMPLE"
  progress_all: "PROGRESS_STATUS"
  progress_main_pipeline_host: "PROGRESS_STATUS/HOST_ID_CONTIGS"
  progress_barcodes: "PROGRESS_STATUS/BARCODE_MARKERS_LONG"
  finished: "samples_finished"


#Unchangable settings (these are deprecated, please ignore) 

# Blast method to use for blastn. Initially had MMseqs2 as well but the memory requirement was 2-5X blastn so dropped given the other high memory steps. 

Blast_method: 'blast'




#  host_species_genomes_idx: "00A_TOP_HOST_SPECIES_PER_SAMPLE/bowtieindex"


# Superflous code to remove after streamlined module selections
# Whether to do KRaken viral classification on the raw sequences 
doviral_raws_classification: 'no' # REMOVE as will be removing Kraken from the pipeline so only one raws analysis is available # need to remove from summary R script first. 
#kraken_database: "/scratch3/odw014/analysis/databases/Kraken/" # REMOVE
#virus_protein_database: "/scratch3/odw014/analysis/databases/Complete_viral/Viral_proteinsdb" # REMOVE



# Does the analysis require a rapid host identification. Will subset LSU and SSU hits for rapid classification of host to then allow for all further analyses to be run
# without host DNA yes/no
Rapid_host_id: 'no'

# Currently not implemented, its purpose is to allow for multiple hosts to be detected simultaneously. Right now only single is viable within the pipeline
#Single/Multiple
Hostdetect: 'Single'

# whether summary results files should be generated. This is required for all additional modules to generate their final results files.
# 'yes/no'
generate_results_summary: 'yes'
